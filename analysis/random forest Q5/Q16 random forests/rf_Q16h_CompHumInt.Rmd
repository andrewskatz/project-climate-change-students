---
title: "Random Forest - Q16 - Computers for Human Intelligence"
author: "Katz et al."
date: "5/19/2021"
output: html_document
---




```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(psych)
library(tidyverse)
library(tidymodels)
library(ranger)
library(purrrlyr)
library(purrr)
library(randomForest)
library(pander)
library(kableExtra)
library(knitr)

```



Script for creating random forests for Q5 and Q16

```{r}
setwd("G:/My Drive/AK Faculty/Research/Projects/project students and climate change/analysis/q27 clustering")

# climate_data <- read_csv("G:/My Drive/AK Faculty/Research/Projects/project students and climate change/data/updated_climate_df_2.csv")


#original data set
#climate_data <- read_csv("G:/My Drive/AK Faculty/Research/Projects/project students and climate change/data/climate_data.csv")

# read in the imputed dataset
climate_imp_long_full <- read_csv("climate_imp_long_40_full_inf_cl_hdb_agg_20210518.csv")
```


```{r}
pub_ds_ver <- 1
climate_data <- climate_imp_long_full %>% filter(.imp == pub_ds_ver)

rm(climate_imp_long_full)

```






# Create a new dataframe for the random forest work

Perform EFA for Q28

```{r}



Q28_vars <- paste0("Q28", letters[1:12])


Q28_df <- climate_data %>% drop_na(Q28_vars) %>% select(Q28_vars)

Q28_fa <- fa(Q28_df, nfactors = 2, fm = "ml", rotate = "oblimin")
print(Q28_fa, cut = 0.3, digits = 3)
# use `print(fa, digits = 3)` to view FLs < .3

```

Calculate Cronbach's alpha for internal consistency reliability

```{r}


FA1 <- c("Q28a", "Q28f", "Q28g", "Q28h", "Q28i", "Q28k")
FA2 <- c("Q28c", "Q28d", "Q28e", "Q28j", "Q28l")

alpha.fa1 <- psych::alpha(Q28_df[FA1])
print(alpha.fa1, digits = 3) #0.91

alpha.fa2 <- psych::alpha(Q28_df[FA2])
print(alpha.fa2, digits = 3) #0.80


```



Add variables for Q28_tech_norm ((Q28a + Q28f + Q28g + Q28h + Q28i + Q28k) / 6) and Q28_social_norm ((Q28c + Q28d + Q28e + Q28j + Q28l) / 5)
```{r}

climate_data <- climate_data %>% 
  mutate(Q28_social = Q28c + Q28d + Q28e + Q28j + Q28l,
         Q28_tech = Q28a + Q28f + Q28g + Q28h + Q28i + Q28k,
         Q28_social_norm = Q28_social / 5,
         Q28_tech_norm = Q28_tech / 6)

```


Perform EFA for Q12

```{r}
Q12_vars <- paste0("Q12", letters[1:9])

Q12_df <- climate_data %>% drop_na(Q12_vars) %>% select(Q12_vars)

Q12_fa <- fa(Q12_df, nfactors = 2, fm = "ml", rotate = "oblimin")
print(Q12_fa, cut = 0.3, digits = 3)

```


```{r}

FA1 <- c("Q12a", "Q12b", "Q12c")
FA2 <- c("Q12d", "Q12e", "Q12f", "Q12g", "Q12h", "Q12i")

alpha.fa1 <- psych::alpha(Q12_df[FA1])
print(alpha.fa1, digits = 3) #0.83

alpha.fa2 <- psych::alpha(Q12_df[FA2])
print(alpha.fa2, digits = 3) #0.90

```



Add variables for Q12 factors (Q12abc_norm (a+b+c)/3 and Q12_defghi_norm = (d+e+f+g+h+i)/6)

```{r}

climate_data <- climate_data %>% 
  mutate(Q12abc = Q12a + Q12b + Q12c,
         Q12defghi = Q12d + Q12e + Q12f + Q12g + Q12h + Q12i,
         Q12abc_norm = Q12abc / 3,
         Q12defghi_norm = Q12defghi / 6)

```

Recode variables
```{r}

climate_data <- climate_data %>% 
  mutate(major = case_when(Q29 == 1 ~ "Aer/Oce",
                           Q29 == 2 ~ "Agr/Biol",
                           Q29 == 3 ~ "Bio",
                           Q29 == 4 ~ "Civ",
                           Q29 == 5 ~ "Che",
                           Q29 == 6 ~ "Con",
                           Q29 == 7 ~ "Comp",
                           Q29 == 8 ~ "Ele",
                           Q29 == 9 ~ "EngPhy",
                           Q29 == 10 ~ "Env/Eco",
                           Q29 == 11 ~ "Ind",
                           Q29 == 12 ~ "Mat",
                           Q29 == 13 ~ "Mec",
                           Q29 == 14 ~ "Min",
                           Q29 == 15 ~ "Nuc",
                           Q29 == 16 ~ "Softw",
                           Q29 == 17 ~ "Str/Arc",
                           Q29 == 18 ~ "Gen"))

```



Remove majors with fewer than 30 students and students with NA for major


```{r}


climate_data %>% count(Q29, sort = TRUE)
climate_data %>% count(major, sort = TRUE)
cutoff <- 30

climate_data <- climate_data %>% 
  add_count(Q29, name = "major_count") %>% 
  filter(major_count > cutoff) %>% 
  filter(!is.na(Q29))

climate_data %>% count(Q29, sort = TRUE)

```


Fill in Q1, Q3, Q5, Q7, Q39 NAs with 0's (the way they were coded, it's ambiguous if an NA is there intentionally by the participant, so this is a conservative assumption to make that an NA actually corresponds to an intentional omission rather than an accidental omission)


```{r}
# prepraing the data
# start with the climate_data dataframe


Q1_vars <- paste0("Q1", letters[1:20])
Q3_vars <- paste0("Q3", letters[1:7])
Q5_vars <- paste0("Q5", letters[1:10])

Q35_vars <- paste0("Q35", letters[1:9])
Q39_vars <- paste0("Q39", letters[1:9])

Q7_vars <- paste0("Q7", letters[1:22])
Q7tally_vars <- paste0("Q7", letters[1:22], "_tally")


#str(climate_data)

### old way to remove disciplines with 
# rf_df <- climate_data %>% 
#   filter(!is.na(Q29)) %>% 
#   filter(Q29 != 15) %>% 
#   filter(Q29 != 14) %>% 
#   filter(Q29 != 9) %>% 
#   filter(Q29 != 2)

rf_df <- climate_data

rf_df %>% group_by(Q29) %>% count()

#rf_df[, Q1_vars]
rf_df <- rf_df %>% mutate_at(vars(Q1_vars), ~replace_na(., 0))
#rf_df[, Q1_vars]

#rf_df[, Q3_vars]
rf_df <- rf_df %>% mutate_at(vars(Q3_vars), ~ replace_na(.,0))
#rf_df[, Q3_vars]

#rf_df[,Q5_vars]

#rf_df[, Q7_vars]
rf_df <- rf_df %>% mutate_at(vars(Q7_vars), ~ replace_na(., 0))
#rf_df[,Q7_vars]

#rf_df[, Q35_vars]
rf_df <- rf_df %>% mutate_at(vars(Q35_vars), ~ replace_na(., 0))
#rf_df[,Q35_vars]

#rf_df[, Q39_vars]
rf_df <- rf_df %>% mutate_at(vars(Q39_vars), ~ replace_na(., 0))
#rf_df[,Q39_vars]
```


Drop individual Q12, Q16, and Q28 items. Q16 items were too similar to Q5 items and Q28 items were factored

```{r}
Q12_vars <- paste0("Q12", letters[1:9])
Q16_vars <- paste0("Q16", letters[1:13])
Q28_vars <- paste0("Q28", letters[1:12])


rf_df <- rf_df %>% drop_na(Q12_vars)
rf_df <- rf_df %>% drop_na(Q16_vars)
rf_df <- rf_df %>% drop_na(Q28_vars)



```

Turn the variables into factors

```{r}
# turning variables into factors


#rf_df <- dmap(rf_df, as.factor)
```


Remove additional columns that will not feed into the random forest algorithm

```{r}

# remove columns with majority na (noticed from visual inspection)
removal_vars <- c("School", "Litho", "major", "Q33_", 
                  "Q37_", "Q38_", "Q39_", "Q40", "abe", "Q16abc", "lm", "dfg", "hijk",
                  "spec", "other", "tally", "ele", "Q16_bin", "_ind")

# Q13, Q14 were originally on the removal list - not sure why - need to add back in

# consider removing "sum" from the above

removal_vars_start <- paste0("^(", paste(removal_vars, collapse="|"), ")")
removal_vars_end <- paste0("(", paste(removal_vars, collapse="|"), ")$")


# create a dataframe called train_df that we'll use for training the random forests
train_df <- rf_df %>% dplyr::select(-matches(removal_vars_start))
train_df <- train_df %>% dplyr::select(-matches(removal_vars_end))

train_df <- train_df %>% 
  dplyr::select(-Q28_tech, -Q28_social, -Q12abc, -Q12defghi)
train_df <- train_df %>% dplyr::select(-Q28_vars)
train_df <- train_df %>% dplyr::select(-Q1_vars)



# remove the original Q7 vars because they were too messy - use tallies instead

# remove other Q5 variables aside from 5e
## probably want to change this to Q16 removals

forest_outcome_var <- "Q16h"

Q16_removals <- Q16_vars[Q16_vars != forest_outcome_var]
# Q16_removals <- c("Q5a","Q5b", "Q5c", "Q5d", "Q5e", "Q5f", "Q5g", "Q5i", "Q5j")

train_df <- train_df %>% dplyr::select(-Q7_vars)
train_df <- train_df %>% dplyr::select(-Q5_vars) # drop Q5 vars for Q16 training
train_df <- train_df %>% dplyr::select(-Q16_removals)
#train_df <- train_df %>% dplyr::select(-Q16_energy)

factor_vars <- c("Q29", "Q32", "Q37")

train_df <- train_df %>% 
  mutate_at(factor_vars, as.factor)
  
  # mutate(Q29 = as.factor(Q29),
  #        Q37 = as.factor(Q37))

train_df %>% count(Q16h)

```


Remove observations with more than 10% obs missing - shouldn't be an issue with imputed dataset

```{r}
# remove rows with too many nas (defined as more than 10% of variables)

# full implementation
train_df <- train_df[!rowSums(is.na(train_df)) > ncol(train_df)*.1,] # reduced df by ~47 to 3127

```




Impute data for the remaining cases

```{r}

#train_df_2 <- mice(train_df, maxit = 2, m = 2, seed = 1)

```

```{r}
# Old method for reading in data

#train_df_2 <- read_csv("G:/My Drive/AK Faculty/Research/Projects/project students and climate change/filtered_climate_survey_20200509.csv")
```




Create random forest for Q5h with tidymodels framework
```{r}
# 
# set.seed(42)




# looking at specific outcome variables
# table(train_df$Q5h)

# create train and test split
# 
# q5h_split <- train_df %>% 
#   initial_split(strata = Q5h)
# 
# q5h_train <- training(q5h_split) # 2440 x 288
# q5h_test <- testing(q5h_split) # 812 x 288


#check to make sure stratified correctly
# table(q5h_train$Q5h)

```



Create the random forest model specification
```{r}

# rf_spec <- rand_forest(mode = "classification") %>% 
#   set_engine("ranger")
# 
# rf_spec

```

Create the random forest model fit

```{r}

# remove other Q5 variables


# rf_fit <- rf_spec %>% 
#   fit(Q5h ~ ., 
#       data = q5h_train
#       )
# 
# rf_fit

```

Check for more variables to remove
```{r}
train_df
```


```{r}
extra_removal_vars <- c(".id", ".imp", "student_id", "full_hdb_cluster", "full_agg_cluster",
                        "cluster_time_rank_hdb", "cluster_avg_hdb", "cluster_avg_agg",
                        "dim1", "dim2")

train_df <- train_df %>% 
  select(-one_of(extra_removal_vars))
```


```{r}

# rf_mod <- randomForest(formula = Q5h ~ ., 
#                        data = q5h_train,
#                        ntree = 1000,
#                        proximity = TRUE,
#                        na.action = na.roughfix,
#                        importance = TRUE)

set.seed(42)


rf_mod <- randomForest(formula = Q16h ~ ., 
                       data = train_df,
                       ntree = 1000,
                       proximity = TRUE,
                       na.action = na.roughfix,
                       importance = TRUE)



```



```{r}

imp <- importance(rf_mod, type = 1, scale = F) # permutation importances 
#(specifying "type = 1" pulls MeanDecreaseAccuracy instead of MeanDecreaseGini)

```

Manual way of collecting variable importance data

```{r}

#row.names(imp)
featureImportance <- data.frame(Feature = row.names(imp), Importance = imp[,1])

p <- featureImportance %>% 
  top_n(30, Importance) %>% 
  ggplot(aes(x = reorder(Feature, -Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "#53cfff", width = 0.65) +
  coord_flip() + 
  theme_light(base_size = 20) +
  theme(axis.title.x = element_text(size = 10, color = "black"),
        axis.title.y = element_blank(),
        axis.text.x  = element_text(size = 10, color = "black"),
        axis.text.y  = element_text(size = 10, color = "black"),
        plot.title = element_text(size = 10, hjust = 0.5)) +
  ggtitle("Random Forest Variable Importance for Career Interest in Working on Computers for Human Intelligence")

p



```




Calculate variable importance and plot using permutation

```{r}
# library(vip)

code_break


```






### Ordinal Forest instead of Regression Random Forest


```{r}
train_df <- train_df %>% 
  mutate(Q16h = as.ordered(Q16h))
```


```{r}
set.seed(42)

# train_df %>% drop_na(Q32, Q33, Q37)

ord_rf_df <- train_df %>% 
  select(-one_of("Q30", "Q31", "Q34", "Q35", "Q36a", "Q36b", "Q38", Q39_vars, "Q40")) %>% 
  drop_na(Q32, Q33, Q37)
```


```{r}
ord_rf_df %>% select(Q16h)

ord_rf_df <- as.data.frame(ord_rf_df)
# default mtry
rf_mod_ord <- ordfor(depvar = "Q16h", 
                     data = ord_rf_df, 
                     perffunction = "proportional")


```



#### Identify top 30 most important variables for ordinal prediction

```{r}
# sort(rf_mod_ord$varimp, decreasing=TRUE)

var_imp <- rf_mod_ord$varimp
# var_imp
# names(var_imp)

var_imp_df <- tibble(variable = names(var_imp),
                     var_importance = var_imp)

```

For ordinal forest variable importance measures

```{r}

p <- var_imp_df %>% 
  top_n(30, var_importance) %>% 
  ggplot(aes(x = reorder(variable, -var_importance), y = var_importance)) +
  geom_bar(stat = "identity", fill = "#53cfff", width = 0.65) +
  coord_flip() + 
  theme_light() +
  labs(x = "Variable",
       y = "Variable Importance") +
  theme(axis.title.x = element_text(size = 10, color = "black"),
        # axis.title.y = element_blank(),
        axis.text.x  = element_text(size = 10, color = "black"),
        axis.text.y  = element_text(size = 10, color = "black"),
        plot.title = element_text(size = 10, hjust = 0.5)) +
  ggtitle("Ordinal Forest Variable Importance for Career Interest in Computers to Emulate Human Intelligence")

p

```


Save the Ordinal Forest model
```{r}
# path <- "G:/My Drive/AK Faculty/Research/Projects/project students and climate change/analysis/random forest Q5/Q16 random forests/"
# 
# file_name <- "rf_mod_ord_Q16h.rds"
# 
# rf_mod_ord %>% write_rds(file = paste0(path, file_name))
```













### Option 2 - Model with predictors from ordinal forest




## Create logistic regression model with Q16k (protecting against cyber-threats) and the following predictors:
Q29
Q18g
Q24f
Q2f
Q4g
Q2e
Q18f
Q7e_wt_sum







### Method using Bayesian ordinal regression


Add major column back to replace Q29 in regression models

```{r}
train_df <- train_df %>% 
  mutate(major = case_when(Q29 == 1 ~ "Aer/Oce",
                           Q29 == 2 ~ "Agr/Biol",
                           Q29 == 3 ~ "Bio",
                           Q29 == 4 ~ "Civ",
                           Q29 == 5 ~ "Che",
                           Q29 == 6 ~ "Con",
                           Q29 == 7 ~ "Comp",
                           Q29 == 8 ~ "Ele",
                           Q29 == 9 ~ "EngPhy",
                           Q29 == 10 ~ "Env/Eco",
                           Q29 == 11 ~ "Ind",
                           Q29 == 12 ~ "Mat",
                           Q29 == 13 ~ "Mec",
                           Q29 == 14 ~ "Min",
                           Q29 == 15 ~ "Nuc",
                           Q29 == 16 ~ "Softw",
                           Q29 == 17 ~ "Str/Arc",
                           Q29 == 18 ~ "Gen"),
         major = as_factor(major))

```


```{r}


str(train_df$major)

# set reference discipline

train_df$major <- fct_relevel(train_df$major, "Mec") 

str(train_df$major)

```

```{r}
get_prior(ordered(Q16h) ~ major + Q18g + Q24f + Q2f + Q4g + Q2e + Q7e_wt_sum + Q18f, 
          data = train_df, family = cumulative("logit"))
```


```{r}
# priors1 <- c(set_prior("normal(0, 1)", class = "b", coef = "Q17b"),
#              set_prior("normal(0, 1)", class = "b", coef = "Q18f"),
#              set_prior("normal(0, 1)", class = "b", coef = "Q18g"),
#              set_prior("normal(0, 1)", class = "b", coef = "Q18l"),
#              set_prior("normal(0, 1)", class = "b", coef = "Q24f"),
#              set_prior("normal(0, 1)", class = "b", coef = "Q2f"),
#              set_prior("normal(0, 1)", class = "b", coef = "Q7p_wt_sum"))

```



```{r}

mod1_ord_Q16h <- brm(formula = ordered(Q16h) ~ major + Q18g + Q24f + Q2f + Q4g + Q2e + Q7e_wt_sum + Q18f,
                     data = train_df,
                     family = cumulative("logit"),
                     cores = parallel::detectCores())

```




```{r}
path <- "G:/My Drive/AK Faculty/Research/Projects/project students and climate change/analysis/random forest Q5/Q16 random forests/"

file_name <- "mod1_ord_Q16h.rds"

mod1_ord_Q16h %>% write_rds(file = paste0(path, file_name))
```

```{r}

mod1_ord_Q16h <- read_rds(file = paste0(path, file_name))

```







```{r}
# no priors
mod1_ord_mono_Q16h <- brm(formula = ordered(Q16h) ~ major + mo(Q18g) + mo(Q24f) + mo(Q4g) + 
                            mo(Q18f) + Q2e + Q2f + Q7e_wt_sum,
                     data = train_df,
                     family = cumulative("logit"),
                     cores = parallel::detectCores())




```

```{r}
path <- "G:/My Drive/AK Faculty/Research/Projects/project students and climate change/analysis/random forest Q5/Q16 random forests/"

file_name <- "mod1_ord_mono_Q16h.rds"

mod1_ord_mono_Q16h %>% write_rds(file = paste0(path, file_name))
```

```{r}

mod1_ord_mono_Q16h <- read_rds(file = paste0(path, file_name))

```


















## Create logistic regression model with Q5d (address climate change in career) and the following predictors:
Q20d
Q18c
Q18k
Q28_tech_norm (afghjk - global warming as technical issue divided by 6)
Q29
Q23a
Q23d



```{r}
set.seed(123)


# subset the original climate dataset AND make 5d (the outcome) and Q29 (major) factors
lr_df <- train_df %>% 
  dplyr::select(Q5d, Q20d, Q18c, Q18k, Q28_tech_norm, Q23a, Q23d, Q29) %>% 
  mutate(Q5d = as.factor(Q5d),
         Q29 = as.factor(Q29))


# make mechanical engineering the reference level for logistic regression models
lr_df$Q29 <- fct_relevel(lr_df$Q29, "13") 

# check the structure of the logistic regression data frame to make sure everything coded correctly
str(lr_df)

```


```{r}
# create the train/test split

# lr_split <- initial_split(lr_df, strata = Q5d)
# lr_train <- training(lr_split)
# lr_test <- testing(lr_split)
# 
# str(lr_train)
```


Create bootstrap resamples


```{r}
# create the model recipe (preprocessing steps)

lr_rec <- recipe(Q5d ~ ., data = lr_df) %>% 
  step_zv(all_numeric()) %>% 
  step_center(all_numeric())

```


```{r}
# prep the recipe
lr_prep <- lr_rec %>% 
  prep()

lr_prep  



```

Create the training data with all the preprocessing and then specify the model

```{r}
## Code just to run one model

lr_juiced <- juice(lr_prep)

glm_spec <- logistic_reg() %>%
  set_engine("glm")

glm_fit <- glm_spec %>%
  fit(Q5d ~ ., data = lr_juiced)

glm_fit



```


old school model creation

Create the bootstrap resamples

```{r}
lr_juiced <- juice(lr_prep)

lr_boot <- bootstraps(lr_juiced,
                      times = 1e4,
                      apparent = TRUE)

#lr_boot
```

Cretae the bootstrap regression model (make 1e4 models on the bootstrap samples)

```{r}
q5d_models <- lr_boot %>% 
  mutate(
    model = purrr::map(splits, ~ glm(Q5d ~ ., data = ., family = binomial(link = 'logit'))),
    coef_info = purrr::map(model, tidy))
```

Collect the bootstrap regression model statistics
```{r}
q5d_coefs <- q5d_models %>% 
  unnest(coef_info)

#q5d_coefs

```

Look at the distribution of logistic regression model coefficients

```{r}
q5d_coefs %>%
  ggplot(aes(estimate)) +
  geom_histogram(alpha = 0.7, fill = "cyan3") +
  facet_wrap(. ~ term, scales = "free")
```


```{r}
q5d_coefs %>%
  ggplot(aes(x = estimate, y = term)) +
  geom_boxplot(alpha = 0.7)
```


```{r}

q5d_coefs %>%
  filter(term %in% c("Q18c", "Q18k", "Q20d", "Q23a", "Q23d", "Q28_tech_norm")) %>% 
  ggplot(aes(x = estimate, y = fct_reorder(term, estimate))) +
  geom_boxplot(alpha = 0.7) +
  geom_vline(xintercept = 0) +
  labs(x = "Coefficient estimate",
       y = "Survey item",
       title = "Distribution of bootstrapped logistic regression coefficient estimates") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_minimal()

```


```{r}
# probably want to drop nuclear engineering Q29_15 (only 3 students) and engineering physics Q29_9 (only 6 students)

q5d_coefs %>%
  filter(!term %in% c("Q18c", "Q18k", "Q20d", "Q23a", "Q28_tech_norm", "(Intercept)", 
                      "Q292", "Q299", "Q2914", "Q2915")) %>% 
  filter(estimate > -10) %>% 
  mutate(term = case_when(term == "Q291" ~ "aero",
                          term == "Q292" ~ "ag/bio",
                          term == "Q293" ~ "bio/bme",
                          term == "Q294" ~ "civil",
                          term == "Q295" ~ "chemical",
                          term == "Q296" ~ "construc",
                          term == "Q297" ~ "comp eng",
                          term == "Q298" ~ "electric",
                          term == "Q2910" ~ "env/eco",
                          term == "Q2911" ~ "ind/sys",
                          term == "Q2912" ~ "material",
                          term == "Q2913" ~ "mech",
                          term == "Q2914" ~ "mining",
                          term == "Q2916" ~ "software",
                          term == "Q2917" ~ "struc/arch",
                          term == "Q2918" ~ "general")) %>% 
  filter(term != "NA") %>% 
  ggplot(aes(x = estimate, y = fct_reorder(term, estimate))) +
  geom_boxplot(alpha = 0.7) +
  geom_vline(xintercept = 0) +
  labs(x = "Coefficient estimate",
       y = "Major",
       title = "Distribution of bootstrapped logistic regression coefficient estimates") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_minimal()

```

Calculate confidence intervals for model coefficients at the 0.01 level to get an estimate of the sampling distribution of each coefficient estimate

```{r}
int_pctl(q5d_models, coef_info, alpha = 0.01) %>% 
  arrange(-.estimate) %>% 
  pander()


```



```{r}
#glm_fit_2 <- glm(Q5d ~ ., data = lr_juiced, family = binomial(link = 'logit'))


```

Run ANOVA on model
```{r}
#anova(glm_fit$fit, test="Chisq")

#pander(anova(glm_fit_2, test="Chisq"))

```



Evaluate model metrics


```{r}

# glm_fit %>% 
#   tidy() %>% 
#   arrange(-estimate) %>% 
#   pander()


```



Evalute the models with resampling (i.e., cross validation to get a distribution of coefficient estimates).
This works by working off the training set and creating analysis/assessment splits depending on the number
of folds in v-fold cross validation.
(set v = 100)

```{r}

set.seed(135)

# 10-fold cross validation
folds <- vfold_cv(lr_juiced, v = 100, strata = Q5d)

```


After creating the folds, need to fit the resamples using fit_resamples.
This creates model metrics for each of the folds


```{r}

set.seed(111)
glm_rs <- glm_spec %>% 
  fit_resamples(
    lr_rec,
    folds,
    metrics = metric_set(roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  )

```


Collect the metrics for the models from the resampling procedure

```{r}
glm_rs %>% 
  collect_metrics() %>% 
  pander()

```


Take a look at the metrics with a plot


```{r}
glm_rs %>% 
  unnest(.predictions) %>% 
  roc_curve(Q5d, .pred_1) %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_line(size = 1.5) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  ) +
  theme_minimal()
```



```{r}

glm_rs %>% 
  unnest(.predictions) %>% 
  roc_curve(Q5d, .pred_1) %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_line(size = 1.5) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  ) +
  theme_minimal()

```




Predict on the test data

```{r}
# str(lr_test)
# 
# glm_fit %>%
#   predict(
#     new_data = bake(lr_prep, lr_test),
#     type = "prob"
#   ) %>%
#   mutate(truth = lr_test$Q5d) %>%
#   roc_auc(truth, .pred_1)

```

